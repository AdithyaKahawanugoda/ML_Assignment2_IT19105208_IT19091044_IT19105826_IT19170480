{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDP-79YlpA-o"
      },
      "source": [
        "**==== 1 - Data import and pre-processing ====**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8G5gpJXLSWl",
        "outputId": "6ab3c15e-a189-4964-b110-a38ac0a48284"
      },
      "outputs": [],
      "source": [
        "# dwonload and unzip dataset\n",
        "!wget https://files.consumerfinance.gov/ccdb/complaints.csv.zip\n",
        "!unzip /content/complaints.csv.zip -d /content/complaints.csv\n",
        "# install NLTK\n",
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZFGdr1P_SNr",
        "outputId": "ed9b84c8-d161-4647-9f84-cd59c11d5dd5"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from io import StringIO\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrmcdWKXXUhv",
        "outputId": "d2244f4d-63c4-4ba0-c8db-4de3c7bb96b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1025010, 19)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load data into a pandas dataframe from csv file\n",
        "df = pd.read_csv(\"/content/complaints.csv/complaints.csv\",low_memory=False)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoX8LMU7XEcF",
        "outputId": "0e5611e1-1fb8-4312-9f7b-49d39084fe0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25010, 19)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# seperate Product & Consumer Complaints columns from initial dataframe\n",
        "col = ['Product', 'Consumer complaint narrative']\n",
        "df = df[col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "execution": {
          "iopub.execute_input": "2022-05-20T18:43:15.734890Z",
          "iopub.status.busy": "2022-05-20T18:43:15.733875Z",
          "iopub.status.idle": "2022-05-20T18:43:28.600496Z",
          "shell.execute_reply": "2022-05-20T18:43:28.599490Z",
          "shell.execute_reply.started": "2022-05-20T18:43:15.734827Z"
        },
        "id": "Mfsrqm4c2k03",
        "outputId": "4822d31a-5566-496d-b256-1830b254d540",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df['Product'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "execution": {
          "iopub.execute_input": "2022-05-20T18:45:10.575837Z",
          "iopub.status.busy": "2022-05-20T18:45:10.574693Z",
          "iopub.status.idle": "2022-05-20T18:45:10.880495Z",
          "shell.execute_reply": "2022-05-20T18:45:10.879217Z",
          "shell.execute_reply.started": "2022-05-20T18:45:10.575768Z"
        },
        "id": "LpIUddPG2k04",
        "outputId": "83c17eb6-9569-4ebe-dc94-a446df2d5df5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# keep only 'not null' complaints rows\n",
        "df = df[pd.notnull(df['Consumer complaint narrative'])] \n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "execution": {
          "iopub.execute_input": "2022-05-20T18:45:41.655246Z",
          "iopub.status.busy": "2022-05-20T18:45:41.654507Z",
          "iopub.status.idle": "2022-05-20T18:45:42.208102Z",
          "shell.execute_reply": "2022-05-20T18:45:42.207189Z",
          "shell.execute_reply.started": "2022-05-20T18:45:41.655202Z"
        },
        "id": "TDTMJ31l2k05",
        "outputId": "5b35ed1d-eae7-4c2a-b022-47c5f8eac90b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# since the dataset is imbalanced\n",
        "# remove products with complaints < 10,000\n",
        "# remove products with complaints > 100,000 \n",
        "counts = df['Product'].value_counts()\n",
        "df = df[~df['Product'].isin(counts[counts < 10000].index)]\n",
        "df = df[~df['Product'].isin(counts[counts > 100000].index)]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove duplicated rows in dataframe\n",
        "df = df.drop_duplicates(keep=\"first\")\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# overwrite colum names into 'Product' & 'Consumer_Complaint'\n",
        "df.columns = ['Product', 'Consumer_Complaint']\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# manually add specific id to each product type\n",
        "cid={'Credit reporting':0, 'Student loan':1, 'Mortgage':2, 'Debt collection':3, 'Vehicle loan or lease':4, 'Credit card or prepaid card':5, 'Checking or savings account':6, 'Money transfer, virtual currency, or money service':7, 'Credit card':8, 'Payday loan, title loan, or personal loan':9, 'Bank account or service':10, 'Credit reporting, credit repair services, or other personal consumer reports':11}\n",
        "df['cid']=df['Product'].map(cid)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Product'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text Preprocessing using NLTK\n",
        "# function definition for stemming, tokenizing, removing stop words and 'XX' values from complaint strings using NLTK\n",
        "# this will be used on TfidfVectorizer\n",
        "stemmer = nltk.stem.SnowballStemmer('english')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "def wordTokenizer(text):\n",
        "   # tokenize words using nltk word_tokenize operation and remove unnecessary characters\n",
        "   tokens = [word for word in nltk.word_tokenize(text) if (len(word) > 3 and len(word.strip('Xx/')) > 2 and len(re.sub('\\d+', '', word.strip('Xx/'))) > 3) ] \n",
        "   # convert complaints data into lowercase\n",
        "   # tf-idf consider same word in different case (lowercase/uppercase/camelcase/mixed) formats as different words. \n",
        "   tokens = map(str.lower, tokens)\n",
        "   # stem and return tokens that are not in stop words list\n",
        "   stems = [stemmer.stem(item) for item in tokens if (item not in stop_words)]\n",
        "   return stems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data distribution among selected product classes\n",
        "fig = plt.figure(figsize=(8,6),dpi=70.0)\n",
        "df.groupby('Product').Consumer_Complaint.count().plot.bar(ylim=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "==== 2. Creating Feature Vector ===="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 3), stop_words='english')\n",
        "# latin-1 is a single-byte encoding which uses the characters 0 through 127, so it can encode half as many characters as latin1. \n",
        "# It's a strict subset of both latin1 and utf8, meaning the bytes 0 through 127 in both latin1 and utf8 encode the same things as they do in ASCII\n",
        "features = tfidf.fit_transform(df.Consumer_Complaint).toarray()\n",
        "labels = df.category_id\n",
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "N = 3\n",
        "for Product, category_id in sorted(category_to_id.items()):\n",
        "  features_chi2 = chi2(features, labels == category_id)\n",
        "  indices = np.argsort(features_chi2[0])\n",
        "  feature_names = np.array(tfidf.get_feature_names_out())[indices]\n",
        "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "  trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
        "  print(\"# '{}':\".format(Product))\n",
        "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
        "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n",
        "  print(\"  . Most correlated trigram:\\n. {}\".format('\\n. '.join(trigrams[-N:])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDP-79YlpA-o"
      },
      "source": [
        "**==== 3. Train, Test and Compare Different ML Models ====**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Consumer_Complaint'], df['Product'], random_state = 0)\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(clf.predict(count_vect.transform([\"This company refuses to provide me verification and validation of debt per my right under the FDCPA. I do not believe this debt is mine.\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "models = [\n",
        "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
        "    LinearSVC(),\n",
        "    MultinomialNB(),\n",
        "    LogisticRegression(random_state=0),\n",
        "]\n",
        "CV = 5\n",
        "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
        "entries = []\n",
        "for model in models:\n",
        "  model_name = model.__class__.__name__\n",
        "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
        "  for fold_idx, accuracy in enumerate(accuracies):\n",
        "    entries.append((model_name, fold_idx, accuracy))\n",
        "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
        "import seaborn as sns\n",
        "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
        "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
        "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_df.groupby('model_name').accuracy.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LinearSVC()\n",
        "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "            xticklabels=category_id_df.Product.values, yticklabels=category_id_df.Product.values)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "for predicted in category_id_df.category_id:\n",
        "  for actual in category_id_df.category_id:\n",
        "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
        "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
        "      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Product', 'Consumer_Complaint']])\n",
        "      print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(features, labels)\n",
        "N = 2\n",
        "for Product, category_id in sorted(category_to_id.items()):\n",
        "  indices = np.argsort(model.coef_[category_id])\n",
        "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n",
        "  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n",
        "  print(\"# '{}':\".format(Product))\n",
        "  print(\"  . Top unigrams:\\n       . {}\".format('\\n       . '.join(unigrams)))\n",
        "  print(\"  . Top bigrams:\\n       . {}\".format('\\n       . '.join(bigrams)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, y_pred, target_names=df['Product'].unique()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of customer-complaints.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
